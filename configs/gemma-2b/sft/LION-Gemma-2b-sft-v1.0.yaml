# Model arguments
model_name_or_path: google/gemma-2b
model_revision: main
tokenizer_name_or_path: philschmid/gemma-tokenizer-chatml # Custom tokenizer with <|im_start|> and <|im_end|> tokens
torch_dtype: bfloat16
use_flash_attention_2: true

# Adjust size of each dataset
train_dataset_mixer:
    Columbia-NLP/OpenHermes-2.5: 1.0
    Columbia-NLP/MetaMathQA: 1.0
    Columbia-NLP/orca-math-word-problems-200k: 1.0
    HuggingFaceH4/ultrachat_200k: 1.0
    HuggingFaceH4/capybara: 1.0
    HuggingFaceH4/deita-10k-v0-sft: 1.0
    Columbia-NLP/ruozhiba_en: 1.0
    Columbia-NLP/Magicoder-Evol-Instruct-110K: 1.0
    Columbia-NLP/SlimOrca-Dedup: 1.0
train_dataset_split: train_sft

# Data training arguments
assistant_bos: "<|im_start|>assistant\n"
assistant_eos: "<|im_end|>"
preprocessing_num_workers: 64
mask_user_labels: false
mask_embed_grad: true
shuffle_train_dataloader: true
use_fast_model: true
dataset_cache_dir: dataset_cache


# SFT trainer config
bf16: true
tf32: true
do_eval: false
evaluation_strategy: "no"
# eval_steps: 2
gradient_accumulation_steps: 2
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 2.0e-05
log_level: info
logging_steps: 5
logging_strategy: steps
lr_scheduler_type: cosine
max_seq_length: 8192
max_steps: -1
num_train_epochs: 3
output_dir: model_checkpoints/LION-Gemma-2b-sft-v1.0
overwrite_output_dir: true
per_device_eval_batch_size: 4
per_device_train_batch_size: 4
push_to_hub: false
remove_unused_columns: true
report_to:
- none
save_strategy: "steps"
save_steps: 5000
save_total_limit: 5
seed: 42
warmup_ratio: 0.1